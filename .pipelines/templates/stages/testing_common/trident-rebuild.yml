parameters:
  - name: sshKeyPath
    type: string
    default: $(Build.SourcesDirectory)/e2e_tests/helpers/key

  - name: userName
    type: string
    default: "testing-user"

  - name: hostIp
    type: string

  - name: runtimeEnv
    displayName: "Runtime environment (host vs container)"
    type: string
    default: "host"
    values:
      - host
      - container

  - name: tridentSourceDirectory
    type: string  

  - name: tridentConfigPath
    type: string 

  - name: deploymentEnvironment  
    type: string
    default: virtualMachine
    values:
      - bareMetal
      - virtualMachine

steps:

  - bash: |
        set -eu
        raidExists=$(sudo yq e '.storage.raid != null' "${{ parameters.tridentConfigPath }}/trident-config.yaml")
        if [ "$raidExists" == "true" ]; then
          echo "Trident config requires Rebuild testing"
          echo "##vso[task.setvariable variable=TEST_REBUILD_RAID]True"
        else
          echo "Trident config does not require Rebuild testing"
          echo "##vso[task.setvariable variable=TEST_REBUILD_RAID]False"
        fi
    displayName: "Check if Trident config requires rebuild-raid testing"

  - ${{ if eq(parameters.deploymentEnvironment, 'bareMetal') }}:
    - bash: |
        set -eux
        # clear the partition table of sdb disk
        ssh -o StrictHostKeyChecking=no -i ${{ parameters.sshKeyPath }} ${{ parameters.userName }}@${{ parameters.hostIp }} "sudo dd if=/dev/zero of=/dev/sdb bs=512 count=1"
        ssh -o StrictHostKeyChecking=no -i ${{ parameters.sshKeyPath }} ${{ parameters.userName }}@${{ parameters.hostIp }} "echo 'label: gpt' | sudo sfdisk /dev/sdb --force"
        # Fail the RAID devices
        python3 $(Build.SourcesDirectory)/e2e_tests/helpers/fail_raid_devices.py \
            --ip-address ${{ parameters.hostIp }} \
            --user-name ${{ parameters.userName }} \
            --keys-file-path ${{ parameters.sshKeyPath }} 
        ssh -o StrictHostKeyChecking=no -i ${{ parameters.sshKeyPath }} ${{ parameters.userName }}@${{ parameters.hostIp }} "sudo reboot" 
      displayName: "Clear the partition table, fail RAID devices of sdb disk and reboot the host"
      condition: and(succeeded(), eq(variables['TEST_REBUILD_RAID'], 'True')) 

  - ${{ if eq(parameters.deploymentEnvironment, 'virtualMachine') }}:
    - bash: |
        set -eux
        echo "Efibootmgr entries in the VM."
        ssh -o StrictHostKeyChecking=no -i ${{ parameters.sshKeyPath }} ${{ parameters.userName }}@${{ parameters.hostIp }} "sudo efibootmgr"
        sudo virsh shutdown virtdeploy-vm-0
        sudo rm -f /var/lib/libvirt/images/virtdeploy-pool/virtdeploy-vm-0-1-volume.qcow2
        sudo qemu-img create -f qcow2 /var/lib/libvirt/images/virtdeploy-pool/virtdeploy-vm-0-1-volume.qcow2 16G
      displayName: "Replace the test disk with a new disk"
      condition: and(succeeded(),eq(variables['TEST_REBUILD_RAID'], 'True')) 

    - bash: |
        set -eux
        echo "Dump the VM before boot order change."
        until sudo virsh dumpxml virtdeploy-vm-0; do 
            sudo virsh list
            sleep 0.1
        done
        echo "Changing the boot order of the VM to boot from sda."
        python3 $(Build.SourcesDirectory)/.pipelines/templates/stages/testing_vm/update-vm-bootorder.py
        echo "Dump the VM after boot order change."
        until sudo virsh dumpxml virtdeploy-vm-0; do 
            sudo virsh list
            sleep 0.1
        done
      displayName: "Change boot order"
      condition: and(succeeded(),eq(variables['TEST_REBUILD_RAID'], 'True'))

    - bash: |
        set -eux

        # Name of the domain
        DOMAIN_NAME="virtdeploy-vm-0"

        # Initial sleep time
        sleep_time=10

        # Check the state of the domain and run the loop
        for (( i=1; i<=30; i++ )); do
            domain_state=$(sudo virsh domstate $DOMAIN_NAME)

            if [[ $domain_state == "shut off" ]]; then
                echo "The domain is shut off. Starting the domain..."
                sudo virsh start $DOMAIN_NAME
                echo "The domain has been started."
                exit 0
            else
                echo "The domain is still running. Waiting for $sleep_time seconds..."
                sleep $sleep_time
                sleep_time=$((sleep_time + 10))
            fi
        done

        echo "The domain did not shut down after 30 attempts."

      displayName: "Restart the VM"
      condition: and(succeeded(),eq(variables['TEST_REBUILD_RAID'], 'True'))

    - bash: |
        set -euxo pipefail
        # Name of the domain
        DOMAIN_NAME="virtdeploy-vm-0"
        
        # Get the VM serial log file path
        VM_SERIAL_LOG=$(sudo virsh dumpxml $DOMAIN_NAME | grep -A 1 console | grep source | cut -d"'" -f2)

        # Wait until the serial log file is created
        until [ -c "$VM_SERIAL_LOG" ]; do
            sleep 0.1
        done

        echo "Found VM serial log file: $VM_SERIAL_LOG"
        echo "VM serial log:"

        # Execute wait_for_login.py to retrieve the serial log
        sudo $(tridentSourceDirectory)/e2e_tests/helpers/wait_for_login.py \
            -d "$VM_SERIAL_LOG" \
            -o ./serial.log \
            -t 500 \
            -v

        # Capture the exit code from wait_for_login.py
        WAIT_FOR_LOGIN_EXITCODE=$?

        # Exit with the captured exit code
        exit $WAIT_FOR_LOGIN_EXITCODE 
      displayName: "Collect Serial Log of the VM"
      condition: eq(variables['TEST_REBUILD_RAID'], 'True')  

  # TODO: Remove after trident-container-verity-testimage is being used for E2E tests (Task #9652). 
  - bash: |
      set -eu
      verityRequired=$(sudo yq e '.storage.verityFilesystems != null' "${{ parameters.tridentConfigPath }}/trident-config.yaml")
      if [ "$verityRequired" == "true" ]; then
        echo "Trident is running from the Runtime OS on the host (verity not enabled for container)."
        echo "##vso[task.setvariable variable=runtimeEnv]host"
      else
        echo "Runtime environment is the same for Provisioning OS and Runtime OS."
        echo "##vso[task.setvariable variable=runtimeEnv]${{ parameters.runtimeEnv }}"
      fi
    displayName: "Define Trident runtime environment"

  - bash: |
      $(Build.SourcesDirectory)/.pipelines/templates/stages/testing_common/scripts/check-ssh-connection.sh \
          "${{ parameters.sshKeyPath }}" \
          "${{ parameters.userName }}" \
          "${{ parameters.hostIp }}" \
          "$(runtimeEnv)"
    displayName: "Check SSH connection after restarting"
    condition: eq(variables['TEST_REBUILD_RAID'], 'True')

  - bash: |
      set -eu
      echo "Running script to rebuild RAID arrays with Trident..."
      tridentRebuildRaidConfigFile="/var/lib/trident/config.yaml"
      python3 $(Build.SourcesDirectory)/e2e_tests/helpers/rebuild_raid.py \
          --ip-address ${{ parameters.hostIp }} \
          --user-name ${{ parameters.userName }} \
          --keys-file-path ${{ parameters.sshKeyPath }} \
          --runtime-env $(runtimeEnv) \
          --trident-config $tridentRebuildRaidConfigFile \
    timeoutInMinutes: 5
    workingDirectory: $(Build.SourcesDirectory)
    displayName: "Test Trident rebuild-raid"
    condition: eq(variables['TEST_REBUILD_RAID'], 'True')
