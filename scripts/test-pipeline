#!/usr/bin/python3

# # # # # # # # # # # # # # # # # # # # #
#             W A R N I N G             #
#    This script is used in pipelines   #
#      and in `make check-pipelines`    #
#       Be careful when modifying!      #
# # # # # # # # # # # # # # # # # # # # #

import argparse
from dataclasses import dataclass
import subprocess
import sys
import tempfile
import json
from typing import Any, Dict


@dataclass
class PipelineMetadata:
    id: int
    project: str = "ECF"
    parameters: Dict[str, Any] = None


pipeline_metadata_map: Dict[str, PipelineMetadata] = {
    "pr": PipelineMetadata(id=5066),
    "pr-e2e": PipelineMetadata(id=5073),
    "pr-e2e-azure": PipelineMetadata(id=5072),
    "ci": PipelineMetadata(id=5067),
    "pre": PipelineMetadata(id=5074),
    "rel": PipelineMetadata(
        id=5075,
        parameters={"specificVersion": "1.0.0"},
    ),
    "testing": PipelineMetadata(id=5078),
    "tester": PipelineMetadata(id=5071),
    "scale": PipelineMetadata(id=5077),
    "scale-official": PipelineMetadata(id=5076),
    "azl-cicd": PipelineMetadata(
        id=5068,
        parameters={
            "baseImagePipelineBuildId": "730139",
            "baseImageArm64PipelineBuildId": "730138",
        },
    ),
    "prism-cicd": PipelineMetadata(id=5069),
    "full-validation": PipelineMetadata(id=5070),
    "publish-crates": PipelineMetadata(id=5324, project="polar"),
}

parser = argparse.ArgumentParser(description="Preview a pipeline run")

parser.add_argument(
    "pipeline",
    type=str,
    help="The pipeline to preview",
    choices=pipeline_metadata_map.keys(),
    nargs="+",
)

parser.add_argument(
    "-b",
    "--branch",
    type=str,
    help="The branch to preview the pipeline for",
    default=None,
)

parser.add_argument(
    "-q", "--quiet", action="store_true", help="Suppress YAML output", default=False
)

parser.add_argument(
    "--parallel",
    action="store_true",
    help="Preview pipelines in parallel",
)

args = parser.parse_args()

if args.parallel and not args.quiet:
    parser.error("--parallel requires --quiet (-q)")


def check_pipeline(pipeline_name: str, selected_branch: str, quiet: bool = False):
    pipeline_metadata = pipeline_metadata_map[pipeline_name]

    if pipeline_metadata.id == -1:
        raise Exception(f"Pipeline '{pipeline_name}' is not defined in the script")

    print(
        f"Checking pipeline '{pipeline_name}' with ID '{pipeline_metadata.id}' on branch '{selected_branch}'",
        file=sys.stderr,
    )

    payload = {
        "previewRun": True,
        "resources": {"repositories": {"self": {"refName": selected_branch}}},
    }

    if pipeline_metadata.parameters:
        payload["templateParameters"] = pipeline_metadata.parameters

    with tempfile.NamedTemporaryFile() as payload_file:
        payload_file.write(json.dumps(payload).encode("utf-8"))
        payload_file.flush()

        cmd = [
            "az",
            "devops",
            "invoke",
            "--org",
            "https://dev.azure.com/mariner-org",
            "--api-version",
            "7.0",
            "--area",
            "pipelines",
            "--resource",
            "runs",
            "--route-parameters",
            f"project={pipeline_metadata.project}",
            f"pipelineId={pipeline_metadata.id}",
            "--http-method",
            "POST",
            "--in-file",
            payload_file.name,
        ]
        output = subprocess.run(
            cmd,
            capture_output=True,
        )

    if output.returncode != 0:
        raise Exception(f"Pipeline preview failed: {output.stderr.decode('utf-8')}")

    print(f"Pipeline '{pipeline_name}' previewed successfully", file=sys.stderr)

    if not quiet:
        out_json = json.loads(output.stdout.decode("utf-8"))
        print(out_json["finalYaml"])


if args.branch:
    selected_branch = args.branch
else:
    selected_branch = (
        "refs/heads/"
        + subprocess.run(
            ["git", "rev-parse", "--abbrev-ref", "HEAD"],
            capture_output=True,
            check=True,
        )
        .stdout.decode("utf-8")
        .strip()
    )

if args.parallel:
    import concurrent.futures

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(check_pipeline, name, selected_branch): name
            for name in args.pipeline
        }
        failures = []
        for future in concurrent.futures.as_completed(futures):
            name = futures[future]
            try:
                future.result()
            except Exception as e:
                print(f"Error previewing pipeline '{name}': {e}", file=sys.stderr)
                failures.append(name)
        if failures:
            print(
                f"Failed pipelines: {', '.join(failures)}",
                file=sys.stderr,
            )
            exit(1)
else:
    for pipeline_name in args.pipeline:
        try:
            check_pipeline(pipeline_name, selected_branch, args.quiet)
        except Exception as e:
            print(f"Error previewing pipeline '{pipeline_name}': {e}", file=sys.stderr)
            exit(1)
